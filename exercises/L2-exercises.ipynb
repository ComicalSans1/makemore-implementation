{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fba8bd-5eea-46c8-aca6-70277f6bb19a",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663f76fd-47cf-46f8-b12b-00f42e3adc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E01\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1399a2-43f3-41ee-be9a-a578667812cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_len = 2\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    context = [0] * context_len\n",
    "\n",
    "    for l in w + '.':\n",
    "        ix = stoi[l]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join([itos[i] for i in context]), \"->\", itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "X.size(), Y.size()\n",
    "# X_e = torch.cat((torch.tensor([i[0] for i in X]), torch.tensor([i[1] for i in X])), 0)\n",
    "X_enc = F.one_hot(X, num_classes=27)\n",
    "\n",
    "X_enc.size()\n",
    "X_enc_c = X_enc.view(Y.size(0), -1).float()\n",
    "X_enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d907f8e-a0fb-4c1a-ac49-d30c39c6a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# W is 54x27, because each element in the 56-long encoded X gets a corresponding tensor of 27 probabilities(log-counts in this case)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abd6299-9790-4f04-b8de-d00946c49382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_enc_c @ W\n",
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(1, keepdim=True)\n",
    "probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9ad47e-6b2e-4007-8757-1320deff4f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0230, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(probs.size(0)), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f78eea1a-6e10-4c03-8b17-26546594f497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0230, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(probs.size(0)), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab01f43-7bcb-4c9b-9c22-ae497ad9de95",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d445a53a-6640-4a20-bf66-083786714d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated\n",
    "\n",
    "context_len = 2\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * context_len\n",
    "\n",
    "    for l in w + '.':\n",
    "        ix = stoi[l]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X_enc = F.one_hot(X, num_classes=27)\n",
    "X_enc_c = X_enc.view(Y.size(0), -1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712f4cb5-6fdc-47b4-8a16-bf5bef8b3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# W is 54x27, because each element in the 56-long encoded X gets a corresponding tensor of 27 probabilities(log-counts in this case)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b8202f0-9224-41f0-a998-a143d9621ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.353471040725708\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "\n",
    "n_iter = 50\n",
    "l_rate = 30\n",
    "\n",
    "for i in range(n_iter):\n",
    "    logits = X_enc_c @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(probs.size(0)), Y].log().mean()\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data -= l_rate * W.grad\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "# final loss: ~2.35\n",
    "# total number of iterations: ~250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995392e1-61cd-4f32-8b81-62f9a5bee082",
   "metadata": {},
   "source": [
    "The loss is noticeably lower than if you used a context of 1, but not by a large margin. The decrease in loss is only ~0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1efe9100-d569-440f-b913-e8a31852f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jmuthanickiyn.\n",
      "cor.\n",
      "aryeshaumiylielyna.\n",
      "afi.\n",
      "raylaperee.\n"
     ]
    }
   ],
   "source": [
    "n_names = 5\n",
    "\n",
    "for i in range(n_names):\n",
    "    w = []\n",
    "    context_1 = [0, 0]\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor([context_1]), num_classes=27).float()\n",
    "        x_enc_c = x_enc.view(1, -1).float()\n",
    "        logits = x_enc_c @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts/counts.sum(1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        context_1 = context_1[1:] + [ix]\n",
    "        w.append(itos[ix])\n",
    "        if ix ==0:\n",
    "            break\n",
    "    print(''.join(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a4c00-4c6c-4328-bac9-56fdb8bd13ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
