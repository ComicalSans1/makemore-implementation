{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bcefcd2-5fc0-4f43-9285-a55c97981389",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. \n",
    "What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c9d314-3b68-4a7c-a8de-640700ad6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "words_train, words_test_val = train_test_split(words, test_size=0.2, random_state=42)\n",
    "words_val, words_test = train_test_split(words_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32554048-4f77-4e03-8794-fbc30194e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3204)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_train), len(words_val), len(words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d8dd8dfc-fddc-4911-8744-72a4af6baac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram\n",
    "\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "def prepare_training_data(corpus):\n",
    "    xs_train = []\n",
    "    ys = []\n",
    "    \n",
    "    for w in corpus:\n",
    "      chs = ['.'] + list(w) + ['.']\n",
    "      for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs_train.append(ix1)\n",
    "        ys.append(ix2)\n",
    "          \n",
    "    \n",
    "    xs = torch.tensor(xs_train)\n",
    "    ys = torch.tensor(ys)\n",
    "    \n",
    "    x_enc = F.one_hot(xs, num_classes=27).float() # converted to float so that you can feed it into the neural net\n",
    "\n",
    "    return xs, ys, x_enc\n",
    "\n",
    "def calculate_loss(xs, ys):\n",
    "    xs = xs if isinstance(xs, torch.Tensor) else torch.tensor(xs)\n",
    "    \n",
    "    logits = xs @ W # log-counts for each next letter\n",
    "    counts = logits.exp() # proper counts for each next letter\n",
    "    probs = counts/counts.sum(1, keepdim=True) # normalized probabilities for each next letter\n",
    "    loss = -probs[torch.arange(probs.size(0)), ys].log().mean() # loss function (vectorized)\n",
    "\n",
    "    return [loss, probs, W]\n",
    "\n",
    "\n",
    "def gradient_descent(xs, ys, n_iter=100, l_rate=50):\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # forward\n",
    "        loss, probs, W = calculate_loss(xs, ys)\n",
    "        \n",
    "        # backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "    \n",
    "        # update\n",
    "        W.data -= l_rate * W.grad\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "def generate_names(n_names, ys):\n",
    "    \n",
    "    for i in range(n_names):\n",
    "        w = []\n",
    "        ix = 0\n",
    "        while True:\n",
    "            ix_enc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "            probs = calculate_loss(ix_enc, ys)[1]\n",
    "            \n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "            w.append(itos[ix])\n",
    "            if ix == 0:\n",
    "                break\n",
    "        print(''.join(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "039dfc1b-7d0f-4504-9b27-2028b007f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.460386276245117\n",
      "ren.\n",
      "cadwikin.\n",
      "nn.\n",
      "aly.\n",
      "jalians.\n"
     ]
    }
   ],
   "source": [
    "xs_train, ys_train, x_enc_train = prepare_training_data(words_train)\n",
    "gradient_descent(x_enc_train, ys_train, 10, 40)\n",
    "generate_names(5, ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f98a01e0-2ff1-4aae-b112-48253bb46ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4538, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_val, ys_val, x_enc_val = prepare_training_data(words_val)\n",
    "calculate_loss(x_enc_val, ys_val)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5d2e4a50-a8d3-46d0-ae0f-7a22b48e9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4640, grad_fn=<NegBackward0>)\n",
      "edeonnta.\n",
      "ki.\n",
      "que.\n",
      "deici.\n",
      "h.\n"
     ]
    }
   ],
   "source": [
    "xs_test, ys_test, x_enc_test = prepare_training_data(words_test)\n",
    "print(calculate_loss(x_enc_test, ys_test)[0])\n",
    "generate_names(5, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7288d6e1-cebb-49e2-8f9c-b1b045926400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram\n",
    "\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)\n",
    "\n",
    "def tri_prepare_training_data(corpus):\n",
    "    \n",
    "    context_len = 2\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * context_len\n",
    "    \n",
    "        for l in w + '.':\n",
    "            ix = stoi[l]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "    \n",
    "    X_enc = F.one_hot(X, num_classes=27)\n",
    "    X_enc_c = X_enc.view(Y.size(0), -1).float()\n",
    "    \n",
    "    return X, Y, X_enc_c\n",
    "\n",
    "def tri_calculate_loss(xs, ys):\n",
    "    xs = xs if isinstance(xs, torch.Tensor) else torch.tensor(xs)\n",
    "    \n",
    "    logits = xs @ W # log-counts for each next letter\n",
    "    counts = logits.exp() # proper counts for each next letter\n",
    "    probs = counts/counts.sum(1, keepdim=True) # normalized probabilities for each next letter\n",
    "    loss = -probs[torch.arange(probs.size(0)), ys].log().mean() # loss function (vectorized)\n",
    "\n",
    "    return [loss, probs, W]\n",
    "\n",
    "\n",
    "def tri_gradient_descent(xs, ys, n_iter=100, l_rate=50):\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # forward\n",
    "        loss, probs, W = calculate_loss(xs, ys)\n",
    "        \n",
    "        # backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "    \n",
    "        # update\n",
    "        W.data -= l_rate * W.grad\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "def tri_generate_names(n_names, ys):\n",
    "    \n",
    "    for i in range(n_names):\n",
    "        w = []\n",
    "        context = [0, 0]\n",
    "        while True:\n",
    "            ix_enc = F.one_hot(torch.tensor([context]), num_classes=27).float()\n",
    "            ix_enc_c = ix_enc.view(1, -1).float()\n",
    "            \n",
    "            probs = calculate_loss(ix_enc_c, ys)[1]\n",
    "            \n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "            context = context[1:] + [ix]\n",
    "            w.append(itos[ix])\n",
    "            if ix == 0:\n",
    "                break\n",
    "        print(''.join(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cc85154f-9660-4bbd-872c-421f168fba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(2.3530, grad_fn=<NegBackward0>)\n",
      "kathey.\n",
      "jo.\n",
      "siyn.\n",
      "kordaia.\n",
      "rea.\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_e_train = tri_prepare_training_data(words_train)\n",
    "\n",
    "# tri_gradient_descent(X_e_train, Y_train, 250, 30)\n",
    "\n",
    "print(\"Loss:\", tri_calculate_loss(X_e_train, Y_train)[0])\n",
    "\n",
    "tri_generate_names(5, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c3ecd-23c2-4ba7-ac66-62fe2303f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (I was too lazy to do the validation stuff here, you get it, i'll get to it later(never))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
